<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>用fastnlp做文本分类与序列标注 | rwqccnuimd的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="文档地址https:&#x2F;&#x2F;fastnlp.readthedocs.io&#x2F; 安装12pip install torchpip install fastnlp  数据预处理 首先需要做的就是将自己的数据装填入fastnlp的Dataset对象中。一个Dataset相当于一个pandas的DataFrame。  构建Vocabulary将字符转化成Id。   使用自己的中文数据集做情感分析Dataset情">
<meta property="og:type" content="article">
<meta property="og:title" content="用fastnlp做文本分类与序列标注">
<meta property="og:url" content="http://example.com/2022/01/04/nlp/fastnlp/index.html">
<meta property="og:site_name" content="rwqccnuimd的博客">
<meta property="og:description" content="文档地址https:&#x2F;&#x2F;fastnlp.readthedocs.io&#x2F; 安装12pip install torchpip install fastnlp  数据预处理 首先需要做的就是将自己的数据装填入fastnlp的Dataset对象中。一个Dataset相当于一个pandas的DataFrame。  构建Vocabulary将字符转化成Id。   使用自己的中文数据集做情感分析Dataset情">
<meta property="og:locale">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200225003007515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpb24xOTkzMDkyNA==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2022-01-04T03:07:25.000Z">
<meta property="article:modified_time" content="2023-03-19T11:43:34.879Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200225003007515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpb24xOTkzMDkyNA==,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="rwqccnuimd的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">rwqccnuimd的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-nlp/fastnlp" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/04/nlp/fastnlp/" class="article-date">
  <time class="dt-published" datetime="2022-01-04T03:07:25.000Z" itemprop="datePublished">2022-01-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NLP/">NLP</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      用fastnlp做文本分类与序列标注
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="文档地址"><a href="#文档地址" class="headerlink" title="文档地址"></a>文档地址</h1><p><a target="_blank" rel="noopener" href="https://fastnlp.readthedocs.io/">https://fastnlp.readthedocs.io/</a></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch</span><br><span class="line">pip install fastnlp</span><br></pre></td></tr></table></figure>

<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><ul>
<li><p>首先需要做的就是将自己的数据装填入fastnlp的<a target="_blank" rel="noopener" href="https://fastnlp.readthedocs.io/zh/latest/tutorials/tutorial_1_data_preprocess.html"><code>Dataset</code></a>对象中。一个<code>Dataset</code>相当于一个pandas的<code>DataFrame</code>。</p>
</li>
<li><p>构建<code>Vocabulary</code>将字符转化成Id。</p>
</li>
</ul>
<h2 id="使用自己的中文数据集做情感分析"><a href="#使用自己的中文数据集做情感分析" class="headerlink" title="使用自己的中文数据集做情感分析"></a>使用自己的中文数据集做情感分析</h2><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>情感分析作为一个二分类任务，首先需要构建一个Dataset，推荐构建的方法为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> DataSet</span><br><span class="line"><span class="comment"># 构建一个dict</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;raw_chars&#x27;</span>: [ <span class="comment"># 一维数组，存放了所有的原始文本</span></span><br><span class="line">        <span class="string">&#x27;这个酒店太辣鸡了吧！&#x27;</span>, </span><br><span class="line">        <span class="string">&#x27;体验还算可以，环境比较安静&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;交通还算是比较便利的，总体打9分&#x27;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;chars&#x27;</span>: [ <span class="comment"># 二维数组，存放的是已经切分为单独的汉字的序列</span></span><br><span class="line">        [<span class="string">&#x27;这&#x27;</span>, <span class="string">&#x27;个&#x27;</span>, <span class="string">&#x27;酒&#x27;</span>, <span class="string">&#x27;店&#x27;</span>, <span class="string">&#x27;太&#x27;</span>, <span class="string">&#x27;辣&#x27;</span>, <span class="string">&#x27;鸡&#x27;</span>, <span class="string">&#x27;了&#x27;</span>, <span class="string">&#x27;吧&#x27;</span>, <span class="string">&#x27;！&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;体&#x27;</span>, <span class="string">&#x27;验&#x27;</span>, <span class="string">&#x27;还&#x27;</span>, <span class="string">&#x27;算&#x27;</span>, <span class="string">&#x27;可&#x27;</span>, <span class="string">&#x27;以&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;环&#x27;</span>, <span class="string">&#x27;境&#x27;</span>, <span class="string">&#x27;比&#x27;</span>, <span class="string">&#x27;较&#x27;</span>, <span class="string">&#x27;安&#x27;</span>, <span class="string">&#x27;静&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;交&#x27;</span>, <span class="string">&#x27;通&#x27;</span>, <span class="string">&#x27;还&#x27;</span>, <span class="string">&#x27;算&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;比&#x27;</span>, <span class="string">&#x27;较&#x27;</span>, <span class="string">&#x27;便&#x27;</span>, <span class="string">&#x27;利&#x27;</span>, <span class="string">&#x27;的&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;总&#x27;</span>, <span class="string">&#x27;体&#x27;</span>, <span class="string">&#x27;打&#x27;</span>, <span class="string">&#x27;9&#x27;</span>, <span class="string">&#x27;分&#x27;</span>],</span><br><span class="line">    ], </span><br><span class="line">    <span class="string">&#x27;raw_words&#x27;</span>: [ <span class="comment"># 一维数组，存放了存放了对原始文本分词(预处理：去除停用词、词干还原....)后形成的一个以空格为分隔符的字符串</span></span><br><span class="line">        <span class="string">&#x27;酒店 辣鸡&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;体验 可以 环境 安静&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;交通 遍历 总体 9分&#x27;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;words&#x27;</span>: [ <span class="comment"># 二维数组，存放了对原始文本分词(预处理：去除停用词、词干还原....)后形成的一个token list</span></span><br><span class="line">        [<span class="string">&#x27;酒店&#x27;</span>, <span class="string">&#x27;辣鸡&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;体验&#x27;</span>, <span class="string">&#x27;可以&#x27;</span>, <span class="string">&#x27;环境&#x27;</span>, <span class="string">&#x27;安静&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;交通&#x27;</span>, <span class="string">&#x27;便利&#x27;</span>, <span class="string">&#x27;总体&#x27;</span>, <span class="string">&#x27;9分&#x27;</span>],</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;seq_len&#x27;</span>: [ <span class="comment"># 一维数组,存放了输入序列的长度，注意是输入的序列,比如情感分析输入的是words的话，那么序列就是[2, 4, 4]</span></span><br><span class="line">        <span class="number">2</span>,</span><br><span class="line">        <span class="number">4</span>,</span><br><span class="line">        <span class="number">4</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;taget&#x27;</span>: [ <span class="comment"># 每一个样本的标签</span></span><br><span class="line">        <span class="string">&#x27;正向&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;负向&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 将dict传入到Dataset中</span></span><br><span class="line">dataset = Dataset(data)</span><br><span class="line">test_dataset = DataSet(test_data)</span><br><span class="line">dev_dataset = DataSet(dev_dataset)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>对不对原始的数据分词、完全取决于自己，</p>
</blockquote>
<h2 id="Vocab"><a href="#Vocab" class="headerlink" title="Vocab"></a>Vocab</h2><p>第二步构建<code>Vocab</code>，将字符数字转化成<code>数字</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Vocabulary</span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.from_dataset(dataset, field_name=<span class="string">&#x27;words&#x27;</span>, no_create_entry_dataset=[dev_dataset, valid_dataset]) <span class="comment"># 根据dataset的words字段来构建vocab，把test dev数据集放到no_create_entry_dataset</span></span><br><span class="line"></span><br><span class="line">target_vocab = Vocabulary(unknown=<span class="literal">None</span>, padding=<span class="literal">None</span>) <span class="comment"># 标签做映射</span></span><br><span class="line">target_vocab.from_dataset(dataset, field_name=<span class="string">&#x27;target&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>关于<code>no_create_dataset</code>，官方文档的表述为：</p>
<p>Vocabulary 中的 no_create_entry ，如果您并不关心具体的原理，您可以<strong>直接采取</strong>以下的建议：在添加来自于<code>非训练集的词的时候将该参数置为True</code>, 或将<code>非训练集数据 传入 no_create_entry_dataset 参数</code>。它们的意义是在接下来的模型会使用pretrain的embedding(包括glove, word2vec, elmo与bert)且会finetune的情况下，如果仅使用来自于train的数据建立vocabulary，会导致只出现在test与dev中的词语无法充分利用到来自于预训练embedding的信息(因为他们 会被认为是unk)，所以在建立词表的时候将test与dev考虑进来会使得最终的结果更好。</p>
<h2 id="DataBundle"><a href="#DataBundle" class="headerlink" title="DataBundle"></a>DataBundle</h2><p>第三步构建<a target="_blank" rel="noopener" href="https://fastnlp.readthedocs.io/zh/latest/fastNLP.io.data_bundle.html"><code>data_bundle</code></a>，将训练集、测试集、验证集、词表集成到一个对象中。关于设立data_bundle的原因官方文档表述如下：</p>
<p>而由于对于同一个任务，训练集，验证集和测试集会<code>共用同一个词表以及具有相同的目标值</code>，所以在fastNLP中我们使用了 DataBundle 来承载同一个任务的多个数据集 DataSet 以及它们的词表 Vocabulary 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP.io.data_bundle <span class="keyword">import</span> DataBundle</span><br><span class="line">data_bundle = Databundle()</span><br><span class="line">data_bundle.set_dataset(train_dataset, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">data_bundle.set_dataset(test_dataset, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">data_bundle.set_dataset(dev_dataset, <span class="string">&#x27;dev&#x27;</span>)</span><br><span class="line">data_bundle.set_vocab(vocab, <span class="string">&#x27;words&#x27;</span>)</span><br><span class="line">data_bundle.set_vocab(target_vocab, <span class="string">&#x27;target&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><p>第四步使用<code>pipeline</code>对<code>data_bundle</code>做处理：主要是批量将数据集中的文本通过vocab转化成数字。针对文本分类任务，其内置了针对不同公开数据集的pipeline，比如针对情感分析数据集，有<code>ChnSentiCorpPipe</code>，<code>IMDBPipe</code>，针对新闻分类数据集，有<code>THUCNewsPipe</code>，比如针对情感分析任务：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP.io.pipe.classification <span class="keyword">import</span> ChnSentiCorpPipe</span><br><span class="line">pipeline = ChnSentiCorpPipe()</span><br><span class="line">data_bundle = pipeline.process(data_bundle)</span><br></pre></td></tr></table></figure>
<p>新的data_bundle里面还有4列，分别是<code>[chars, raw_chars, target, seq_len]</code>，其中<code>chars</code>里面包含了所有转化后的数字。<br><code>ChnSentiCorpPipe</code>会对dataset中的<code>raw_chars</code>与<code>target</code>进行处理。针对raw_chars的话就是进行简单的字符切分，因此如果要用分好词的情感分析数据，需要把分词好的数据放到<code>raw_chars</code>。<br>不过针对自己的数据集，建议使用<a target="_blank" rel="noopener" href="https://fastnlp.readthedocs.io/zh/latest/_modules/fastNLP/io/pipe/classification.html#AGsNewsPipe"><code>CLSBasePipe</code></a>也可以选择自己重新写一个类继承<code>CLSBasePipe</code>,<code>CLSBasePipe</code>会对dataset中的<code>raw_chars</code>与<code>target</code>进行处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP.io.pipe.classification <span class="keyword">import</span> BasePipe</span><br><span class="line">pipeline = BasePipe()</span><br><span class="line">data_bundle = pipeline.process(data_bundle)</span><br></pre></td></tr></table></figure>
<p>新的data_bundle生成了3列，分别是<code>[words, raw_words, seq_len]</code>。其中<code>words</code>中存放了转化后的数字。</p>
<h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>第五步位选择词向量模型，可以用中文词向量也可以选择bert</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP.embeddings <span class="keyword">import</span> StaticEmbedding</span><br><span class="line"><span class="comment"># word2vec</span></span><br><span class="line">embed = StaticEmbedding(data_bundle.get_vocab(<span class="string">&#x27;words&#x27;</span>), model_dir_or_name=<span class="string">&#x27;cn-char-fastnlp-100d&#x27;</span>)</span><br><span class="line"><span class="comment"># bert</span></span><br><span class="line"><span class="keyword">from</span> fastNLP.embeddings <span class="keyword">import</span> BertEmbedding</span><br><span class="line">embed = BertEmbedding(data_bundle.get_vocab(<span class="string">&#x27;words&#x27;</span>), model_dir_or_name=<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> fastNLP.modules <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTMMaxPoolCls</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed, num_classes, hidden_size=<span class="number">400</span>, num_layers=<span class="number">1</span>, dropout=<span class="number">0.3</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = embed</span><br><span class="line"></span><br><span class="line">        self.lstm = LSTM(self.embed.embedding_dim, hidden_size=hidden_size//<span class="number">2</span>, num_layers=num_layers,</span><br><span class="line">                         batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout_layer = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, chars, seq_len</span>):</span>  <span class="comment"># 这里的名称必须和DataSet中相应的field对应，比如之前我们DataSet中有chars，这里就必须为chars</span></span><br><span class="line">        <span class="comment"># chars:[batch_size, max_len]</span></span><br><span class="line">        <span class="comment"># seq_len: [batch_size, ]</span></span><br><span class="line">        chars = self.embed(chars)</span><br><span class="line">        outputs, _ = self.lstm(chars, seq_len)</span><br><span class="line">        outputs = self.dropout_layer(outputs)</span><br><span class="line">        outputs, _ = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.fc(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;pred&#x27;</span>:outputs&#125;  <span class="comment"># [batch_size,], 返回值必须是dict类型，且预测值的key建议设为pred</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = BiLSTMMaxPoolCls(word2vec_embed, <span class="built_in">len</span>(data_bundle.get_vocab(<span class="string">&#x27;target&#x27;</span>)))</span><br></pre></td></tr></table></figure>

<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> CrossEntropyLoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> AccuracyMetric</span><br><span class="line"></span><br><span class="line">loss = CrossEntropyLoss()</span><br><span class="line">optimizer = Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">metric = AccuracyMetric()</span><br><span class="line">device = <span class="number">0</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>  <span class="comment"># 如果有gpu的话在gpu上运行，训练速度会更快</span></span><br><span class="line"></span><br><span class="line">trainer = Trainer(train_data=data_bundle.get_dataset(<span class="string">&#x27;train&#x27;</span>), model=model, loss=loss,</span><br><span class="line">                  optimizer=optimizer, batch_size=<span class="number">32</span>, dev_data=data_bundle.get_dataset(<span class="string">&#x27;dev&#x27;</span>),</span><br><span class="line">                  metrics=metric, device=device)</span><br><span class="line">trainer.train()  <span class="comment"># 开始训练，训练完成之后默认会加载在dev上表现最好的模型</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Tester</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Performance on test is:&quot;</span>)</span><br><span class="line">tester = Tester(data=data_bundle.get_dataset(<span class="string">&#x27;test&#x27;</span>), model=model, metrics=metric, batch_size=<span class="number">64</span>, device=device)</span><br><span class="line">tester.test()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>fastnlp也给出了如何以<a target="_blank" rel="noopener" href="https://fastnlp.readthedocs.io/zh/latest/tutorials/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#ps"><code>词</code></a>为基本单位来做数据预处理</p>
</blockquote>
<blockquote>
<p>fastnlp是怎么针对不同的任务做数据转换的</p>
</blockquote>
<h2 id="使用自己的数据集做序列标注"><a href="#使用自己的数据集做序列标注" class="headerlink" title="使用自己的数据集做序列标注"></a>使用自己的数据集做序列标注</h2><p>序列标注任务以<code>chars</code>作为最基本的单元，因此需要的是最核心的<code>raw_chars</code>与<code>target</code>两列。</p>
<blockquote>
<p>调试的时候可以先用小批量的数据，train test dev用同样的数据。！</p>
</blockquote>
<blockquote>
<p>data_bundle里面可以没有<code>vocab</code>因为<code>pipeline</code>会去生成一个vocab</p>
</blockquote>
<h1 id="序列标注使用bert准确率为0问题排查"><a href="#序列标注使用bert准确率为0问题排查" class="headerlink" title="序列标注使用bert准确率为0问题排查"></a>序列标注使用bert准确率为0问题排查</h1><p>使用自带的微博数据集能够正确得运行</p>
<h2 id="用微博的数据集再加上自己的代码来弄"><a href="#用微博的数据集再加上自己的代码来弄" class="headerlink" title="用微博的数据集再加上自己的代码来弄"></a>用微博的数据集再加上自己的代码来弄</h2><ul>
<li>使用微博的data_bundle+_NERPipe结果为0</li>
<li>使用微博的data_bundle+WeiboNERPipe结果为0</li>
<li>使用Bert的<code>www</code>模型也不行</li>
<li>把学习率调小，使用官方文档里面默认的：<strong>有效果</strong></li>
<li>设置优化器为空</li>
</ul>
<blockquote>
<p>把学习率调小,调整到跟官方给的demo一样！自己当时用的学习率是按照那个word2vec的来的，官方的用bert的学习率为<code>2e-5</code>，也就是<code>0.0001</code></p>
</blockquote>
<p>关于学习率的文章，可以看这个论文<code>How to How to Fine-Tune BERT for Text Classification</code>，推荐的就是<code>2e-5</code></p>
<p><img src="https://img-blog.csdnimg.cn/20200225003007515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpb24xOTkzMDkyNA==,size_16,color_FFFFFF,t_70" alt="学习率"></p>
<h1 id="使用自己的数据进行情感分析"><a href="#使用自己的数据进行情感分析" class="headerlink" title="使用自己的数据进行情感分析"></a>使用自己的数据进行情感分析</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> DataSet</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Vocabulary</span><br><span class="line"><span class="keyword">from</span> fastNLP.io.data_bundle <span class="keyword">import</span> DataBundle</span><br><span class="line"><span class="keyword">from</span> fastNLP.io <span class="keyword">import</span> CWSPipe, ChnSentiCorpPipe</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> fastNLP.modules <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> fastNLP.embeddings <span class="keyword">import</span> BertEmbedding</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> CrossEntropyLoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> AccuracyMetric</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Tester</span><br><span class="line"><span class="keyword">from</span> fastNLP.io.model_io <span class="keyword">import</span> ModelSaver</span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pkl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;dataset/train.labeled.csv&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 保留两列</span></span><br><span class="line">df = df[[<span class="string">&#x27;微博中文内容&#x27;</span>, <span class="string">&#x27;情感倾向&#x27;</span>]]</span><br><span class="line"><span class="comment"># 去除空值</span></span><br><span class="line">df = df.dropna()</span><br><span class="line"><span class="comment"># 列重新命名</span></span><br><span class="line">df.columns = [<span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"><span class="comment"># 过滤label</span></span><br><span class="line">df = df[df[<span class="string">&#x27;label&#x27;</span>].isin([<span class="string">&#x27;-1&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>])]</span><br><span class="line">df[<span class="string">&#x27;label&#x27;</span>] = df[<span class="string">&#x27;label&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="comment"># 重新索引</span></span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置最大字符数</span></span><br><span class="line">max_len = <span class="number">410</span></span><br><span class="line"><span class="comment"># 处理content字段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_content</span>(<span class="params">sentence</span>):</span></span><br><span class="line">    sentence = <span class="built_in">str</span>(sentence).strip()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sentence) &gt;= max_len:</span><br><span class="line">        sentence = sentence[<span class="number">0</span>: max_len]</span><br><span class="line">    <span class="keyword">return</span> sentence</span><br><span class="line">df[<span class="string">&#x27;content&#x27;</span>] = df[<span class="string">&#x27;content&#x27;</span>].apply(process_content)</span><br><span class="line"><span class="comment"># 再次过滤内容为空的数据</span></span><br><span class="line">df = df[df[<span class="string">&#x27;content&#x27;</span>] != <span class="string">&#x27;&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据</span></span><br><span class="line">train_idx, test_idx, _, _  = train_test_split(<span class="built_in">list</span>(<span class="built_in">range</span>(df.shape[<span class="number">0</span>])), df[<span class="string">&#x27;label&#x27;</span>], test_size=<span class="number">0.1</span>, random_state=<span class="number">32</span>)</span><br><span class="line">df[<span class="string">&#x27;type&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">df.loc[train_idx, <span class="string">&#x27;type&#x27;</span>] = <span class="string">&#x27;train&#x27;</span></span><br><span class="line">df.loc[test_idx, <span class="string">&#x27;type&#x27;</span>] = <span class="string">&#x27;test&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造fastnlp数据格式</span></span><br><span class="line">df[<span class="string">&#x27;chars&#x27;</span>] = df[<span class="string">&#x27;content&#x27;</span>].apply(<span class="keyword">lambda</span> x: [<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>(x)])</span><br><span class="line">df[<span class="string">&#x27;seq_len&#x27;</span>] = df[<span class="string">&#x27;chars&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x))</span><br><span class="line">df[<span class="string">&#x27;target&#x27;</span>] = df[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据加载成功&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_dataset</span>(<span class="params">_type</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成dataset</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    _df = df[df[<span class="string">&#x27;type&#x27;</span>] == _<span class="built_in">type</span>]</span><br><span class="line">    dataset = DataSet(&#123;</span><br><span class="line">        <span class="string">&#x27;raw_chars&#x27;</span>: _df[<span class="string">&#x27;content&#x27;</span>].values,</span><br><span class="line">        <span class="string">&#x27;chars&#x27;</span>: _df[<span class="string">&#x27;chars&#x27;</span>].values,</span><br><span class="line">        <span class="string">&#x27;seq_len&#x27;</span>: _df[<span class="string">&#x27;seq_len&#x27;</span>].values,</span><br><span class="line">        <span class="string">&#x27;target&#x27;</span>: _df[<span class="string">&#x27;target&#x27;</span>].values</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">train_dataset = gen_dataset(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">test_dataset = gen_dataset(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">valid_dataset = gen_dataset(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建vocab</span></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line"><span class="comment">#  将验证集或者测试集在建立词表是放入no_create_entry_dataset这个参数中。</span></span><br><span class="line">vocab.from_dataset(train_dataset, field_name=<span class="string">&#x27;chars&#x27;</span>, no_create_entry_dataset=[valid_dataset, test_dataset])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建databundle</span></span><br><span class="line">data_bundle = DataBundle(&#123;<span class="string">&#x27;chars&#x27;</span>: vocab&#125;, &#123;<span class="string">&#x27;train&#x27;</span>: train_dataset, <span class="string">&#x27;test&#x27;</span>: test_dataset, <span class="string">&#x27;valid&#x27;</span>: valid_dataset&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理databunble</span></span><br><span class="line">data_bundle = ChnSentiCorpPipe().process(data_bundle)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTMMaxPoolCls</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed, num_classes, hidden_size=<span class="number">400</span>, num_layers=<span class="number">1</span>, dropout=<span class="number">0.3</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = embed</span><br><span class="line">        self.lstm = LSTM(self.embed.embedding_dim, hidden_size=hidden_size//<span class="number">2</span>, num_layers=num_layers,</span><br><span class="line">                         batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout_layer = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, chars, seq_len</span>):</span>  <span class="comment"># 这里的名称必须和DataSet中相应的field对应，比如之前我们DataSet中有chars，这里就必须为chars</span></span><br><span class="line">        <span class="comment"># chars:[batch_size, max_len]</span></span><br><span class="line">        <span class="comment"># seq_len: [batch_size, ]</span></span><br><span class="line">        chars = self.embed(chars)</span><br><span class="line">        outputs, _ = self.lstm(chars, seq_len)</span><br><span class="line">        outputs = self.dropout_layer(outputs)</span><br><span class="line">        outputs, _ = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.fc(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;pred&#x27;</span>:outputs&#125;  <span class="comment"># [batch_size,], 返回值必须是dict类型，且预测值的key建议设为pred</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">char_vocab = data_bundle.get_vocab(<span class="string">&#x27;chars&#x27;</span>)</span><br><span class="line"><span class="comment"># 加载Bert</span></span><br><span class="line">bert_embed = BertEmbedding(char_vocab, model_dir_or_name=<span class="string">&#x27;cn&#x27;</span>, auto_truncate=<span class="literal">True</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">model = BiLSTMMaxPoolCls(bert_embed, <span class="built_in">len</span>(data_bundle.get_vocab(<span class="string">&#x27;target&#x27;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line"><span class="comment"># 定义评价指标</span></span><br><span class="line">metric = AccuracyMetric()</span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">device = <span class="number">0</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>  <span class="comment"># 如果有gpu的话在gpu上运行，训练速度会更快</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义trainer</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">n_epochs = <span class="number">30</span></span><br><span class="line">trainer = Trainer(train_data=data_bundle.get_dataset(<span class="string">&#x27;train&#x27;</span>), model=model, loss=loss,</span><br><span class="line">                  optimizer=optimizer, batch_size=batch_size, dev_data=data_bundle.get_dataset(<span class="string">&#x27;test&#x27;</span>),</span><br><span class="line">                  metrics=metric, device=device, n_epochs=n_epochs)</span><br><span class="line"><span class="comment"># 开始训练，训练完成之后默认会加载在dev上表现最好的模型</span></span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Performance on test is:&quot;</span>)</span><br><span class="line">tester = Tester(data=data_bundle.get_dataset(<span class="string">&#x27;test&#x27;</span>), model=model, metrics=metric, batch_size=batch_size, device=device)</span><br><span class="line">tester.test()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model_path = <span class="string">&#x27;./save_models/bert_senti.pt&#x27;</span></span><br><span class="line">saver = ModelSaver(model_path)</span><br><span class="line">saver.save_pytorch(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./save_models/data_bundle.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> wp:</span><br><span class="line">    pkl.dump(data_bundle, wp)</span><br></pre></td></tr></table></figure>

<h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> DataSet</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Vocabulary</span><br><span class="line"><span class="keyword">from</span> fastNLP.io.data_bundle <span class="keyword">import</span> DataBundle</span><br><span class="line"><span class="keyword">from</span> fastNLP.io <span class="keyword">import</span> CWSPipe, ChnSentiCorpPipe</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> fastNLP.modules <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> fastNLP.embeddings <span class="keyword">import</span> BertEmbedding</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> CrossEntropyLoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> AccuracyMetric</span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Tester</span><br><span class="line"><span class="keyword">from</span> fastNLP.io.model_io <span class="keyword">import</span> ModelSaver, ModelLoader</span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> pkl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备</span></span><br><span class="line">device = <span class="number">0</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>  <span class="comment"># 如果有gpu的话在gpu上运行，训练速度会更快</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTMMaxPoolCls</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed, num_classes, hidden_size=<span class="number">400</span>, num_layers=<span class="number">1</span>, dropout=<span class="number">0.3</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = embed</span><br><span class="line">        self.lstm = LSTM(self.embed.embedding_dim, hidden_size=hidden_size//<span class="number">2</span>, num_layers=num_layers,</span><br><span class="line">                         batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout_layer = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, chars, seq_len</span>):</span>  <span class="comment"># 这里的名称必须和DataSet中相应的field对应，比如之前我们DataSet中有chars，这里就必须为chars</span></span><br><span class="line">        <span class="comment"># chars:[batch_size, max_len]</span></span><br><span class="line">        <span class="comment"># seq_len: [batch_size, ]</span></span><br><span class="line">        chars = self.embed(chars)</span><br><span class="line">        outputs, _ = self.lstm(chars, seq_len)</span><br><span class="line">        outputs = self.dropout_layer(outputs)</span><br><span class="line">        outputs, _ = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.fc(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;pred&#x27;</span>:outputs&#125;  <span class="comment"># [batch_size,], 返回值必须是dict类型，且预测值的key建议设为pred</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载data_bundle</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./save_models/data_bundle.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    data_bundle = pkl.load(fp)</span><br><span class="line"></span><br><span class="line">char_vocab = data_bundle.get_vocab(<span class="string">&#x27;chars&#x27;</span>)</span><br><span class="line"><span class="comment"># 加载Bert</span></span><br><span class="line">bert_embed = BertEmbedding(char_vocab, model_dir_or_name=<span class="string">&#x27;cn&#x27;</span>, auto_truncate=<span class="literal">True</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">model = BiLSTMMaxPoolCls(bert_embed, <span class="built_in">len</span>(data_bundle.get_vocab(<span class="string">&#x27;target&#x27;</span>)))</span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line"><span class="comment"># 有GPU的情况下用这个方式加载</span></span><br><span class="line">params = ModelLoader.load_pytorch_model(<span class="string">&#x27;./save_models/bert_senti.pt&#x27;</span>)</span><br><span class="line"><span class="comment"># 没有GPU的情况下用这个方式加载，先放到CPU上面去</span></span><br><span class="line">params = torch.load(<span class="string">&#x27;./save_models/bert_senti.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line">model.load_state_dict(params)</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line">max_len = <span class="number">410</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">sentence</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    预测一句话的情感倾向</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sentence = <span class="built_in">str</span>(sentence).strip()</span><br><span class="line">    sentence = sentence[<span class="number">0</span> :max_len]</span><br><span class="line">    chars = <span class="built_in">list</span>(sentence)</span><br><span class="line">    test_dataset = DataSet(data=&#123;<span class="string">&#x27;chars&#x27;</span>: [chars, ]&#125;)</span><br><span class="line">    data_bundle.get_vocab(<span class="string">&#x27;chars&#x27;</span>).index_dataset(test_dataset, field_name=<span class="string">&#x27;chars&#x27;</span>, new_field_name=<span class="string">&#x27;token_ids&#x27;</span>)</span><br><span class="line">    test_data = torch.tensor(test_dataset[<span class="string">&#x27;token_ids&#x27;</span>]).to(device)</span><br><span class="line">    seq_len = torch.tensor([test_data.size()[<span class="number">1</span>], ]).to(device)</span><br><span class="line">    pred_index = model(test_data, seq_len)[<span class="string">&#x27;pred&#x27;</span>].argmax().detach().cpu().numpy().tolist()</span><br><span class="line">    pred_label = data_bundle.get_vocab(<span class="string">&#x27;target&#x27;</span>).to_word(pred_index)</span><br><span class="line">    <span class="keyword">return</span> pred_label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./dataset/test.csv&#x27;</span>)</span><br><span class="line">df = df[df[<span class="string">&#x27;微博中文内容&#x27;</span>] != <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">df[<span class="string">&#x27;预测结果&#x27;</span>] = df[<span class="string">&#x27;微博中文内容&#x27;</span>].apply(predict)</span><br><span class="line">df.to_csv(<span class="string">&#x27;dataset/test.labeled.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li>利用fastnlp做的第一个中文情感分析Demo. <a target="_blank" rel="noopener" href="https://blog.csdn.net/yingdajun/article/details/106825834">https://blog.csdn.net/yingdajun/article/details/106825834</a></li>
<li>Bert微调技巧实验大全-How to Fine-Tune BERT for Text Classification. <a target="_blank" rel="noopener" href="https://blog.csdn.net/lion19930924/article/details/104469944">https://blog.csdn.net/lion19930924/article/details/104469944</a></li>
<li>Adam和学习率衰减（learning rate decay）. <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wuliytTaotao/p/11101652.html">https://www.cnblogs.com/wuliytTaotao/p/11101652.html</a></li>
<li>spacy做命名实体识别. <a target="_blank" rel="noopener" href="https://blog.harumonia.moe/fastnlp-and-spacy/">https://blog.harumonia.moe/fastnlp-and-spacy/</a></li>
<li></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/04/nlp/fastnlp/" data-id="cl85d5h8t00k4i99ofbquc0y1" data-title="用fastnlp做文本分类与序列标注" class="article-share-link">Teilen</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/01/05/bigdata/win_install/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          windows上配置大数据相关组件
        
      </div>
    </a>
  
  
    <a href="/2022/01/03/other/jupyterlab_remote_visit/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">服务器上配置jupyter lab远程访问</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Kategorien</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/CV/">-CV</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Django/">Django</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leetcode/">Leetcode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SQL/">SQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/django/">django</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/">leetcode</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/sql/">sql</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%9A%E5%8A%A1/">业务</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87/">业务指标</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AD%E4%BF%A1%E8%AF%81%E5%88%B8/">中信证券</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%85%B6%E4%BB%96/">其他</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%85%B6%E4%BB%96/Django/">Django</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flume/">Flume</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/">毕业论文</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/">毕业设计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A7%8B%E6%8B%9B/">秋招</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A7%8B%E6%8B%9B/%E9%9D%A2%E8%AF%95/">面试</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AF%95/">笔试</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/">统计学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%87%91%E5%B1%B1%E5%AE%9E%E4%B9%A0/">金山实习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%93%B6%E8%81%94%E5%AE%9E%E4%B9%A0/">银联实习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95/">面试</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">-leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CV/" rel="tag">CV</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django/" rel="tag">Django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flume/" rel="tag">Flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hbase/" rel="tag">Hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/" rel="tag">SQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%9A%E5%8A%A1/" rel="tag">业务</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%AD%E4%BF%A1%E8%AF%81%E5%88%B8/" rel="tag">中信证券</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%85%B6%E4%BB%96/" rel="tag">其他</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E4%B9%A0/" rel="tag">实习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E4%BD%9C/" rel="tag">工作</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" rel="tag">数据集</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/" rel="tag">时间序列预测</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/" rel="tag">毕业论文</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/" rel="tag">毕业设计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%8B%E6%8B%9B/" rel="tag">秋招</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AC%94%E8%AF%95/" rel="tag">笔试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" rel="tag">统计学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%87%91%E5%B1%B1%E5%AE%9E%E4%B9%A0/" rel="tag">金山实习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%B6%E8%81%94%E5%AE%9E%E4%B9%A0/" rel="tag">银联实习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag">面试</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/leetcode/" style="font-size: 10px;">-leetcode</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Django/" style="font-size: 13.33px;">Django</a> <a href="/tags/Flume/" style="font-size: 10px;">Flume</a> <a href="/tags/Hbase/" style="font-size: 10px;">Hbase</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/SQL/" style="font-size: 18.89px;">SQL</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/leetcode/" style="font-size: 15.56px;">leetcode</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/%E4%B8%9A%E5%8A%A1/" style="font-size: 13.33px;">业务</a> <a href="/tags/%E4%B8%AD%E4%BF%A1%E8%AF%81%E5%88%B8/" style="font-size: 11.11px;">中信证券</a> <a href="/tags/%E5%85%B6%E4%BB%96/" style="font-size: 20px;">其他</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 13.33px;">大数据</a> <a href="/tags/%E5%AE%9E%E4%B9%A0/" style="font-size: 16.67px;">实习</a> <a href="/tags/%E5%B7%A5%E4%BD%9C/" style="font-size: 11.11px;">工作</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 10px;">数据集</a> <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/" style="font-size: 10px;">时间序列预测</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 12.22px;">机器学习</a> <a href="/tags/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/" style="font-size: 14.44px;">毕业论文</a> <a href="/tags/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">毕业设计</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 12.22px;">爬虫</a> <a href="/tags/%E7%A7%8B%E6%8B%9B/" style="font-size: 13.33px;">秋招</a> <a href="/tags/%E7%AC%94%E8%AF%95/" style="font-size: 12.22px;">笔试</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" style="font-size: 12.22px;">统计学</a> <a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 11.11px;">论文</a> <a href="/tags/%E9%87%91%E5%B1%B1%E5%AE%9E%E4%B9%A0/" style="font-size: 13.33px;">金山实习</a> <a href="/tags/%E9%93%B6%E8%81%94%E5%AE%9E%E4%B9%A0/" style="font-size: 10px;">银联实习</a> <a href="/tags/%E9%9D%A2%E8%AF%95/" style="font-size: 17.78px;">面试</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/05/14/other/time_series_pred/">时间序列预测</a>
          </li>
        
          <li>
            <a href="/2023/05/01/other/customer_analysis_in_python/">用户价值分析</a>
          </li>
        
          <li>
            <a href="/2022/06/15/other/PEP8/">PEP-8: Python代码规范</a>
          </li>
        
          <li>
            <a href="/2022/05/29/other/fanqie/">番茄工作法</a>
          </li>
        
          <li>
            <a href="/2022/05/20/citics/neo4j_query_optimize/">neo4j查询调优</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>