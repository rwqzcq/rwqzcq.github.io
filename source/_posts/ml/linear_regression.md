---
title: 线性回归
date: 2021-05-10 14:32:47
tags:
categories:
 - 机器学习 
---

# 简介

回归分析（regression analysis)是研究一组随机变量(y1 ，y2 ，…，yi)和另一组(x1，x2，…，xk)变量之间的关系的统计分析方法。
通常用于一个或多个自变量（也成解释变量、预测变量）来预测应变量（也称因变量、校变量或结果变量）的场景中。
譬如，体重和饮食，运动、睡眠之间有怎样的关系？施肥，浇水对树苗的成长关系？近期的广告投入提高了销售额么？日常生活中，淘宝相关产品，同类别歌曲的推荐中都用到了回归的原理。

# 前提

线性回归的前提假设之一是残差必须`服从独立正态分布`，
线性回归的损失函数mse就是：在某个(u,σ^2)下，使得服从正态分布的ε取得现有样本εi的概率最大从而推算出来的损失函数的表达式。

# 假设

1. 假设特征与标签之间满足线性关系
2. 误差项（ε）之间应相互独立。（比如时间序列数据常常发生误差项不是相互独立的情况，比如今天的数据会收到昨天和前天的数据的影响）
3. 自变量之间应相互独立
4. 误差项（ε）应呈正态分布。

# 最小二乘法

最小二乘法以`估计值与观测值的平方和`作为损失函数，在误差服从正态分布的前提下（这一点容易被忽视），与极大似然估计的思想在本质上是相同。
也就是说不是我们特意去选择mse作为线性回归的损失函数而是因为我们假设`误差服从正态分布`，使用`极大似然法（最大化误差项目为εi的样本总体出现的概率最大）来求解参数`，
进一步经过推导之后得到的mse的公式而已。

# 参考链接

1. https://zhuanlan.zhihu.com/p/66519299