---
title: 逻辑回归
date: 2021-05-10 14:33:10
tags:
categories:
 - 机器学习 
---

# 简介

## 一句话概括

逻辑回归假设数据服从`伯努利分布`，通过`极大化似然函数`的方法，运用`梯度下降`来求解参数，来达到将`数据二分类`的目的。

## 假设

逻辑回归假设数据服从伯努利分布，伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是p,抛中为负面的概率是1−p。
在逻辑回归这个模型里面是假设 hθ(x) 为样本为正的概率，1−hθ(x)为样本为负的概率。

## 如何做分类

逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。
y值确实是一个`连续的变量`。逻辑回归的做法是`划定一个阈值`，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。
阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

# 线性回归与逻辑回归的区别

1. 线性回归的变量服从0-1正态分布，而逻辑回归的变量服从伯努利分布。
2. 线性回归要求`因变量`是`连续性数值变量`，而逻辑回归要求因变量是`分类型变量`。
3. 线性回归要求自变量和因变量呈线性关系，而逻辑回归不要求自变量和因变量呈线性关系。
4. 逻辑回归是分析因变量`取某个值的概率`与自变量的关系，而线性回归是直接分析因变量与自变量的关系。

# 正则化

降低过拟合。过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。
原因是模型过于复杂，过分的去拟合数据的噪声。
正则化项是对模型参数添加先验，使得模型复杂度较小，对于噪声扰动相对较小。
在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则。

## L1
![](https://img-blog.csdnimg.cn/20190826120845495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzMjMxNTcz,size_16,color_FFFFFF,t_70)
L1 正则的本质其实是为模型增加了“模型参数服从零均值拉普拉斯分布”这一先验知识。
L1能够精确地评价每一个特征对于损失函数结果的影响程度，对于影响程度小的特征，它直接把他们的系数θi归零！

## L2
L2 正则的本质其实是为模型增加了“模型参数服从零均值正态分布”这一先验知识。
L2 正则化中增加所有权重 w 参数的平方之和，逼迫所有 w 尽可能趋向零但不为零（L2 的导数趋于零）。
因为在未加入 L2 正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些 w 值非常大。为此，L2 正则化的加入就惩罚了权重变大的趋势。

# 优缺点

## 优点
1. 形式简单，模型的`可解释性`非常好。从`特征的权重`可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
2. 训练速度较快。分类的时候，计算量仅仅只和`特征的数目`相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
3. 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。

## 缺点
1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比
10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
3. 较难去处理非线性数据。
4. 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

# 参考链接
1. https://www.cnblogs.com/ModifyRong/p/7739955.html
2. https://blog.csdn.net/sinat_33231573/article/details/99709837
3. https://zhuanlan.zhihu.com/p/345566088
4. https://zhuanlan.zhihu.com/p/74874291